{
 "metadata": {
  "name": "",
  "signature": "sha256:0ead71efd31b7ee3072c977940dd5feef873cfb0611f3a4abe1338c883b99605"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For every classifier, there are __a lot of__ parameters to be optimized in Scikit-Learn to get an optimal accuracy for the classifier in the cross-validation dataset. You could always try and fail and see which parameters work for best but it is time consuming(computation, storage and reporting) and cumbersome in general if you want to handle the parameter search by yourself. Consider these parameters an extra burden when you try to choose an optimal classifier out of multiple classifiers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scikit-learn provides a grid search with cross validation so that when you do the parameter search in `GridSearchCV`, it automatically finds the best parameters tested on a cross-validation so you do not have to search the parameters in different folds."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One thing to note is that it may take a lot of time to do on a regular machine if you have a lot of parameters to try as the number of parameters will be a combinatorial of those parameters. So you may want to use a powerful and multicore machine to do the grid search in the parameter space. I.e: get an aws machine and make it does the work rather than yourself. Icing on the cake is that, you could just kill it when you are done."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline \n",
      "\n",
      "import matplotlib as mlp\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "from sklearn import cross_validation\n",
      "from sklearn import datasets\n",
      "from sklearn import ensemble\n",
      "from sklearn import grid_search\n",
      "from sklearn import metrics\n",
      "import time\n",
      "\n",
      "plt.style.use('fivethirtyeight')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "california_housing = datasets.california_housing.fetch_california_housing()\n",
      "california_housing_data = california_housing['data']\n",
      "california_housing_labels = california_housing['target']# 'target' variables\n",
      "california_housing_feature_names = california_housing['feature_names']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(california_housing_data,\n",
      "                                                    california_housing_labels,\n",
      "                                                    test_size=0.2,\n",
      "                                                    random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to create a grid search object, it is necessary to pass the estimator, parameters to optimize for that estimator, optionally you could pass `n_job` in order to parallellize the parameter search. Grid search in parameter space is embarrasingly parallel by nature as different parameter combinations are independent from each other. You could also pass different scoring functions if you want to use another scoring function to optimize the best parameters for your classifier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "param_grid = {\n",
      "              'learning_rate': [0.1, 0.05, 0.01],\n",
      "              'max_depth': [4, 6],\n",
      "              'min_samples_leaf': [3, 9, 15],\n",
      "              'n_estimators': [1000, 2000, 3000],\n",
      "              }\n",
      "\n",
      "est = ensemble.GradientBoostingRegressor()\n",
      "\n",
      "\n",
      "start_time = time.time()\n",
      "gs_cv = grid_search.GridSearchCV(est, param_grid, n_jobs=4).fit(X_train, y_train)\n",
      "end_time = time.time()\n",
      "\n",
      "print('It took {:.2f} seconds'.format(end_time - start_time))\n",
      "# best hyperparameter setting\n",
      "gs_cv.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you want to get scores for each different parameter combination, `grid_scores_` attribute of GridSearch object provides a nice way to provide all of the scores for each parameter combination."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_cv.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs_cv.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could look at various metrics as well, passing `scoring` optional parameter to `GridSearchCV`. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}