{
 "metadata": {
  "name": "",
  "signature": "sha256:6100d1f001db144fc7d4174ade911385c37ccb17dde9a6d660e6e821fcb17f64"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we learn something so far, that is machine learning is not only applying an algorithm and get the predictions.(Not so much commodization after all) It has quite a lot of different and moving parts for a given problem. Steps(feature extraction, feature selection, classifier, evaluation) follows a sequential order. \n",
      "\n",
      "Would it be perfect if we could wrap all of the steps in one object and then do the parameter search(i.e. grid parameter search) for cross validation in that object. Pipeline in Scikit-Learn exactly wraps these sequential steps for us. Further, if we have two estimators in the __pipeline__(say we apply PCA to reduce dimension in the input and then apply SVM), we would need only once to use the `fit` function in the estimator. Pipeline automatically applies the correct steps for you to get the correct output at the end of the pipeline.\n",
      "\n",
      "Still not convinced? What if I say, serializing one `pipeline` instead of serializing `vectorizer`, `feature_selector` and `classifier` separately and then deploying into production makes much easier.(More on to this in the next notebook) This was a quick win for the pipeline. Let's see how one might use it in classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import csv\n",
      "import json\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import os\n",
      "from sklearn import cross_validation \n",
      "from sklearn import datasets\n",
      "from sklearn import decomposition\n",
      "from sklearn import ensemble\n",
      "from sklearn import feature_extraction\n",
      "from sklearn import feature_selection\n",
      "from sklearn import grid_search\n",
      "from sklearn import metrics\n",
      "from sklearn import naive_bayes\n",
      "from sklearn import pipeline\n",
      "from sklearn import tree\n",
      "\n",
      "import seaborn as sns\n",
      "\n",
      "pd.set_option('display.max_columns', None)\n",
      "\n",
      "_DATA_DIR ='data'\n",
      "_SPAM_DATA_PATH = os.path.join(_DATA_DIR, 'SMSSpamCollection')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### [Dataset Explanation](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
      "- A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: [Web Link]. \n",
      "- A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: [Web Link]. \n",
      "- A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at [Web Link]. \n",
      "- Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at: [Web Link]. This corpus has been used in the following academic researches: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(_SPAM_DATA_PATH, sep='\\t', header=None, names=['Label', 'Text'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Label</th>\n",
        "      <th>Text</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  ham</td>\n",
        "      <td> Go until jurong point, crazy.. Available only ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  ham</td>\n",
        "      <td>                     Ok lar... Joking wif u oni...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> spam</td>\n",
        "      <td> Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  ham</td>\n",
        "      <td> U dun say so early hor... U c already then say...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  ham</td>\n",
        "      <td> Nah I don't think he goes to usf, he lives aro...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "  Label                                               Text\n",
        "0   ham  Go until jurong point, crazy.. Available only ...\n",
        "1   ham                      Ok lar... Joking wif u oni...\n",
        "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
        "3   ham  U dun say so early hor... U c already then say...\n",
        "4   ham  Nah I don't think he goes to usf, he lives aro..."
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nothing very fancy. Let's convert the pandas dataframe into a numpy matrix, and then do a cross validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = (df.Label == 'ham').values.astype(int)\n",
      "X = df.Text.values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n",
      "                                                    y,\n",
      "                                                    test_size=0.2,\n",
      "                                                    random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Let's Create our pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipe = pipeline.Pipeline([('vect', feature_extraction.text.CountVectorizer()),\n",
      "                          ('tfidf', feature_extraction.text.TfidfTransformer()),\n",
      "                          (\"bernoulli\", naive_bayes.BernoulliNB()),\n",
      "                         ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipe.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        st...e_idf=True)), ('bernoulli', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Functions that are applicable to estimators are also applicable to Pipelines. That is one of the most powerful premise of the pipeline after all. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.accuracy_score(pipe.predict(X_test), y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "0.98116591928251118"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By now, we know that this score is not very meaningful, let's look at the confusion matrix!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.heatmap(metrics.confusion_matrix(pipe.predict(X_test), y_test), annot=True,  fmt='');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD9CAYAAAAf46TtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJR+zABsTU8wAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x109dab050>"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(metrics.classification_report(pipe.predict(X_test), y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.87      1.00      0.93       139\n",
        "          1       1.00      0.98      0.99       976\n",
        "\n",
        "avg / total       0.98      0.98      0.98      1115\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is actually pretty good. We classify all of the spam as spam whereas couple of normal messages get into spam folder. Not ideal but pretty good for our first try. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Upto here, you may start convincing yourself how pipeline would be much better and useful and easier than applying each separate component in the machine learning pipeline. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Grid Search in Pipeline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One might apply grid search to the pipeline similar to what we did on the estimator as well. Then one may ask, how do we pass parameters for vectorizer, feature selector and classifier. In the grid search of an estimator, this would be easy as you could pass a `dictionary` which has the keys for the parameters and the parameters as lists that you want to optimize. However, the things in `pipeline` is not that straightforward. First, what if two estimators share the same parameter name and you want to give different values in the search space. What if you do not want to pass any list of parameter to one and pass to the other one? In order to handle this ambiguity, `pipeline` accepts parameters in the form of `{name}__{parameter}` in the dictionary where the `{name}` is the name of the step that you are passing to the pipeline and the parameter is the parameter name that you want to optimize in that step."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4> Why double leading underscore?</h4>\n",
      "> <b>__double_leading_underscore</b>: when naming a class attribute, invokes name mangling (inside class FooBar, __boo becomes _FooBar__boo; ) \n",
      "\n",
      "From [PEP 8](http://legacy.python.org/dev/peps/pep-0008/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the pipeline, we have only two steps: `pca` and `gbf`. There will be two different parameter family in the parameters dictionary; the ones that start with `vectorizer` and the ones start with `gbf`, both of the parameter names will be followed by `__`(double underscore) and then the original parameter name. For an example, if I want to pass `n_estimators` as a parameter to `GradientBoostingClassifier` which was named `gbf` in the pipeline, I need to pass as `gbf_n_estimators`. Since I will be looking at different values for each parameter, I will pass a list in the values of that dictionary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = dict(vect__max_df=[0.5, 1.0],\n",
      "              vect__max_features=[None, 10000, 200000],\n",
      "              vect__ngram_range=[(1, 1), (1, 2)],\n",
      "              tfidf__use_idf=[True, False],\n",
      "              tfidf__norm=['l1', 'l2'],\n",
      "              bernoulli__alpha=[0, .5, 1],\n",
      "              bernoulli__binarize=[None, .1, .5],\n",
      "              bernoulli__fit_prior=[True, False]\n",
      "             )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After preparing the parameters, we are ready to pass this parameter and the pipeline to the `RandomizedSearchCV`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wait, what? I thought we are using `GridSearchCV` like we did earlier notebook. Oh, that. Instead of searching __all__ the parameters in the parameter space, `RandomizedSearchCV` makes a randomized search. The total number of parameters that have been tried for optimizing the search parameters is determined by an optional parameter `n_iter`. If you set the `n_iter` to be 20, then 20 different total number of parameter combinations will be tried out in optimizing the parameter search space. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_iter_search = 100\n",
      "random_search = grid_search.RandomizedSearchCV(pipe, param_distributions=params,\n",
      "                                   n_iter=n_iter_search)\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_search.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "RandomizedSearchCV(cv=None, error_score='raise',\n",
        "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        st...e_idf=True)), ('bernoulli', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))]),\n",
        "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
        "          param_distributions={'vect__ngram_range': [(1, 1), (1, 2)], 'vect__max_df': [0.5, 1.0], 'tfidf__use_idf': [True, False], 'tfidf__norm': ['l1', 'l2'], 'bernoulli__binarize': [None, 0.1, 0.5], 'bernoulli__fit_prior': [True, False], 'bernoulli__alpha': [0, 0.5, 1], 'vect__max_features': [None, 10000, 200000]},\n",
        "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
        "          scoring=None, verbose=0)"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_search.best_estimator_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=0.5, max_features=200000, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        ..._idf=True)), ('bernoulli', BernoulliNB(alpha=0.5, binarize=0.1, class_prior=None, fit_prior=False))])"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_search.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "[mean: 0.97151, std: 0.00259, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.98070, std: 0.00208, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.87054, std: 0.00110, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.90913, std: 0.00143, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.87009, std: 0.00091, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.61813, std: 0.00818, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.94570, std: 0.00033, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.96410, std: 0.00281, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.89567, std: 0.00888, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.63765, std: 0.01297, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.95961, std: 0.00529, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.93942, std: 0.00199, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86246, std: 0.00088, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.97734, std: 0.00114, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86942, std: 0.00337, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.93673, std: 0.00362, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.89230, std: 0.00867, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.86112, std: 0.00181, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.94436, std: 0.00116, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.94279, std: 0.00199, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.89522, std: 0.00913, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.13843, std: 0.00206, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.94279, std: 0.00199, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.89971, std: 0.00272, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.63765, std: 0.01297, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.98385, std: 0.00145, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.94346, std: 0.00387, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.90240, std: 0.00217, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.13731, std: 0.00110, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.97913, std: 0.00095, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.87054, std: 0.00110, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.81557, std: 0.00330, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.95490, std: 0.00359, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.94301, std: 0.00223, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.23873, std: 0.00547, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.14988, std: 0.00206, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.43684, std: 0.00165, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.65425, std: 0.00290, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.97913, std: 0.00095, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.81378, std: 0.00626, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.94458, std: 0.00086, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.65425, std: 0.00290, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.86830, std: 0.00028, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.94570, std: 0.00033, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.94570, std: 0.00033, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 10000}]"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But we know that the classification accuracy is not the best metric to optimize as we already have a quite high classification accuracy. Let's optimize for `f1` score by passing optional `scoring='f1'`!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_search = grid_search.RandomizedSearchCV(pipe, param_distributions=params,\n",
      "                                   n_iter=n_iter_search, scoring='f1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_search.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "RandomizedSearchCV(cv=None, error_score='raise',\n",
        "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        st...e_idf=True)), ('bernoulli', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))]),\n",
        "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
        "          param_distributions={'vect__ngram_range': [(1, 1), (1, 2)], 'vect__max_df': [0.5, 1.0], 'tfidf__use_idf': [True, False], 'tfidf__norm': ['l1', 'l2'], 'bernoulli__binarize': [None, 0.1, 0.5], 'bernoulli__fit_prior': [True, False], 'bernoulli__alpha': [0, 0.5, 1], 'vect__max_features': [None, 10000, 200000]},\n",
        "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
        "          scoring='f1', verbose=0)"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_search.best_estimator_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        s...e_idf=True)), ('bernoulli', BernoulliNB(alpha=0.5, binarize=0.1, class_prior=None, fit_prior=True))])"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_search.grid_scores_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "[mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.96794, std: 0.00012, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92930, std: 0.00052, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.94679, std: 0.00113, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.91889, std: 0.00220, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.96675, std: 0.00257, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.35998, std: 0.01141, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.96753, std: 0.00077, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.95109, std: 0.00116, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.96437, std: 0.00135, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.96030, std: 0.00178, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.96716, std: 0.00065, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.89204, std: 0.00501, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.96794, std: 0.00012, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92930, std: 0.00052, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.01283, std: 0.00260, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.98900, std: 0.00096, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.96437, std: 0.00135, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92614, std: 0.00049, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.73956, std: 0.01196, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.93579, std: 0.00580, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92955, std: 0.00058, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.96626, std: 0.00129, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.02396, std: 0.00821, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92962, std: 0.00032, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.91889, std: 0.00220, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.48267, std: 0.00687, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.96437, std: 0.00135, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.04099, std: 0.00437, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.96612, std: 0.00113, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.93161, std: 0.00207, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.98385, std: 0.00145, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.11813, std: 0.00810, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.61689, std: 0.00626, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92626, std: 0.00063, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.97728, std: 0.00291, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.89763, std: 0.00189, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.98900, std: 0.00118, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.22477, std: 0.00919, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.93161, std: 0.00207, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.93418, std: 0.00527, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': False, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': None},\n",
        " mean: 0.98900, std: 0.00118, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': False, 'vect__max_features': 200000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': None, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.95496, std: 0.00247, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': None},\n",
        " mean: 0.93059, std: 0.00060, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 1.0, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.96275, std: 0.00171, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': None, 'bernoulli__alpha': 0, 'bernoulli__fit_prior': False, 'vect__max_features': 10000},\n",
        " mean: 0.93040, std: 0.00047, params: {'vect__ngram_range': (1, 2), 'vect__max_df': 0.5, 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.1, 'bernoulli__alpha': 0.5, 'bernoulli__fit_prior': True, 'vect__max_features': 10000},\n",
        " mean: 0.92951, std: 0.00016, params: {'vect__ngram_range': (1, 1), 'vect__max_df': 0.5, 'tfidf__use_idf': False, 'tfidf__norm': 'l1', 'bernoulli__binarize': 0.5, 'bernoulli__alpha': 1, 'bernoulli__fit_prior': True, 'vect__max_features': 10000}]"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "best_pipe = pipeline.Pipeline([('vect', feature_extraction.text.CountVectorizer(ngram_range=(1, 1), max_df=1.0, max_features=20000)),\n",
      "                               ('tfidf', feature_extraction.text.TfidfTransformer(use_idf=True, norm='l2')),\n",
      "                               (\"bernoulli\", naive_bayes.BernoulliNB(binarize=0.1, alpha=.5, fit_prior=True)),\n",
      "                              ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "best_pipe.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=20000, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        s...e_idf=True)), ('bernoulli', BernoulliNB(alpha=0.5, binarize=0.1, class_prior=None, fit_prior=True))])"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sns.heatmap(metrics.confusion_matrix(best_pipe.predict(X_test), y_test), annot=True,  fmt='');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD9CAYAAAAf46TtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x109cea110>"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(metrics.classification_report(best_pipe.predict(X_test), y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.89      0.99      0.94       144\n",
        "          1       1.00      0.98      0.99       971\n",
        "\n",
        "avg / total       0.99      0.98      0.98      1115\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a small improvement for humanity and also for us sadly :(. But we could do better with a different classifier, with more parameters,\n",
      "and even using grid search instead of randomized! There are paramters that wait for us to optimize!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}